<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Fan Chen" />

<meta name="date" content="2022-02-09" />

<title>Interpreting factors with bff(), the Best Feature Function</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>






<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">Interpreting factors with bff(), the Best Feature Function</h1>
<h4 class="author">Fan Chen</h4>
<h4 class="date">2022-02-09</h4>



<div id="intro" class="section level2">
<h2>Intro</h2>
<p>In post-clustering analysis, the Best Feature Function (BFF) is useful in selecting representative features for each cluster, especially in the case when additional covariates are available for each feature. For example, consider a social network of <span class="math inline">\(n\)</span> users partitioned into <span class="math inline">\(k\)</span> clusters, and each user possess a series of text document (covariates). We want to summarize words that are representative to each cluster. The BFF is suitable for this type of task.</p>
<p>This document describes the intuition behind the BFF as a follow-up step after the <code>vsp</code> (vintage specral clustering) and touches several technical issues regarding implementation.</p>
</div>
<div id="methodology" class="section level2">
<h2>Methodology</h2>
<p>For simplicity, we consider a symmetric square input matrix (e.g., the adjacency matrix of an undirected graph); the analysis on rectangular input is also supported by <code>bff()</code>. Given a data matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, the <code>vsp</code> returns an approximation with factorization, <span class="math inline">\(ZBY^T\)</span>, where <span class="math inline">\(B \in \mathbb{R}^{k \times k}\)</span> is low-rank, and <span class="math inline">\(Y \in \mathbb{R}^{n \times k}\)</span> encodes the loadings of each feature (i.e., columns of <span class="math inline">\(A\)</span>) with respect to clusters. In particular, when <span class="math inline">\(A\)</span> is the adjacency matrix of an undirected Blockmodel graph, each row of <span class="math inline">\(Y\)</span> decodes the block (cluster) membership of the vertex (feature). Generally, the loading <span class="math inline">\(Y_{ij}\)</span> (for <span class="math inline">\(i=1,...,n\)</span> and <span class="math inline">\(j=1,...,k\)</span>) can be interpreted as an membership measure of the <span class="math inline">\(i\)</span>-th feature to the <span class="math inline">\(j\)</span>-th cluster. <!-- When normalized, it is also an estimator of mixed membership. --></p>
<p>Now, suppose in addition that we have covariates on each feature, <span class="math inline">\(D \in \mathbb{R}^{n \times p}\)</span>, where <span class="math inline">\(p\)</span> is the dimension of covariates. For example, <span class="math inline">\(D\)</span> can be a document-term matrix, where all text data associated with <span class="math inline">\(i\)</span>-th (for <span class="math inline">\(i=1,...,n\)</span>) feature are pooled into a meta document, and <span class="math inline">\(p\)</span> under this circumstance is the size of corpus (i.e., total number of words/terms), and <span class="math inline">\(D_{il}\)</span> is the frequency of word <span class="math inline">\(l\)</span> (for <span class="math inline">\(l=1,...,p\)</span>) appearing in the <span class="math inline">\(i\)</span>-th document.</p>
<p>The BFF then uses <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> to produce an assessment of covariates “best” for each cluster. To start with, suppose both <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> has only non-negative entries.Define the importance, <span class="math inline">\(I \in \mathbb{R}^{p \times k}\)</span>, of the <span class="math inline">\(l\)</span>-th covariate to the <span class="math inline">\(j\)</span>-th cluster by the average of <span class="math inline">\(l\)</span>-th covariate (the <span class="math inline">\(l\)</span>-th columns of <span class="math inline">\(D\)</span>), weighted by the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[I_{lj} = \sum_{j=1}^n D_{jl} Y_{ij}, \text{ for } l=1,...,p,i=1,...n,\]</span></p>
<p>or compactly (side note: the cross product <span class="math inline">\(\langle D,Y \rangle\)</span> is defined as <span class="math inline">\(D^T Y\)</span> as in convention),</p>
<p><span class="math display">\[I=\langle D,Y \rangle.\]</span></p>
<p>As such, a higher value in <span class="math inline">\(I_{lj}\)</span> indicates more significant importance. BFF selects the “best” covariates for each cluster according to the <span class="math inline">\(j\)</span>-th (for <span class="math inline">\(j=1, ..., k\)</span>) column of <span class="math inline">\(I\)</span>.</p>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
<p>Below are a few notes on the implementation of BFF:</p>
<ul>
<li><p><strong>Positive skewness</strong>. When <span class="math inline">\(D\)</span> is a document-term matrix (a.k.a., bags of words), it holds that all elements are non-negative. However, there is absolutely no guarantee that <span class="math inline">\(Y\)</span> has all non-negative entries. This motivates the positive-skew transformation, i.e., we flip the signs of those columns of <span class="math inline">\(Y\)</span> that have negative sample third <a href="https://en.wikipedia.org/wiki/Moment_(mathematics)">moment</a>.</p></li>
<li><p><strong>Handling negative elements</strong>. For now, we undergo a rather ad-hoc solution to the existence of negative elements in <span class="math inline">\(Y\)</span> – pretending they have little effects. In the above importance calculation, negative weights (<span class="math inline">\(Y_{ij}&lt;0\)</span>) are equally treated as <span class="math inline">\(-1\)</span>. In other words, the negative elements result in some subtractions/reduction/contraction in the importance metrics.</p></li>
<li><p><strong>Weight normalization</strong>. In BFF, we utilize the <span class="math inline">\(Y\)</span> matrix as a way of weighting covariates (in <span class="math inline">\(D\)</span>). It is therefore natrual to expect the columns of <span class="math inline">\(Y\)</span> to be (akin to) some probability distributions, i.e., the probability to select one member from the cluster at random. Recall also that the columns of <span class="math inline">\(Y\)</span> all have (or close to) unit <span class="math inline">\(\ell_2\)</span>-norm. Hence, additional transformation is needed: we normalized <span class="math inline">\(Y\)</span> by column. In particular, this is done separately for positive and negative elements.</p></li>
<li><p><strong>Variance stabilization</strong>. If we model <span class="math inline">\(I_{lj}\)</span> with Poisson rate model <span class="math inline">\(\text{Poisson}(\lambda_{lj})\)</span>, the sample mean and variance are coupled (i.e., both have the expectation of <span class="math inline">\(\lambda_{lj}\)</span>). In order to standardize our importance measure <span class="math inline">\(I\)</span>, we need to decouple these two statistics. Performing a square-root transformation, <span class="math inline">\(f(x)=\sqrt{x}\)</span>, does the trick; it stabilizes the sampling variance, which becomes nearly constant.</p></li>
</ul>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
