<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Fan Chen" />

<meta name="date" content="2025-08-20" />

<title>Interpreting factors with bff(), the Best Feature Function</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Interpreting factors with bff(), the Best
Feature Function</h1>
<h4 class="author">Fan Chen</h4>
<h4 class="date">2025-08-20</h4>



<div id="intro" class="section level2">
<h2>Intro</h2>
<p>In post-clustering analysis, the Best Feature Function (BFF) is
useful in selecting representative features for each cluster, especially
in the case when additional covariates are available for each feature.
For example, consider a social network of <span class="math inline">\(n\)</span> users partitioned into <span class="math inline">\(k\)</span> clusters, and each user possess a
series of text document (covariates). We want to summarize words that
are representative to each cluster. The BFF is suitable for this type of
task.</p>
<p>This document describes the intuition behind the BFF as a follow-up
step after the <code>vsp</code> (vintage spectral clustering) and
touches several technical issues regarding implementation.</p>
</div>
<div id="methodology" class="section level2">
<h2>Methodology</h2>
<p>For simplicity, we consider a symmetric square input matrix (e.g.,
the adjacency matrix of an undirected graph); the analysis on
rectangular input is also supported by <code>bff()</code>. Given a data
matrix <span class="math inline">\(A \in \mathbb{R}^{n \times
n}\)</span>, the <code>vsp</code> returns an approximation with
factorization, <span class="math inline">\(ZBY^T\)</span>, where <span class="math inline">\(B \in \mathbb{R}^{k \times k}\)</span> is
low-rank, and <span class="math inline">\(Y \in \mathbb{R}^{n \times
k}\)</span> encodes the loadings of each feature (i.e., columns of <span class="math inline">\(A\)</span>) with respect to clusters. In
particular, when <span class="math inline">\(A\)</span> is the adjacency
matrix of an undirected block model graph, each row of <span class="math inline">\(Y\)</span> decodes the block (cluster) membership
of the vertex (feature). Generally, the loading <span class="math inline">\(Y_{ij}\)</span> (for <span class="math inline">\(i=1,...,n\)</span> and <span class="math inline">\(j=1,...,k\)</span>) can be interpreted as an
membership measure of the <span class="math inline">\(i\)</span>-th
feature to the <span class="math inline">\(j\)</span>-th cluster.
<!-- When normalized, it is also an estimator of mixed membership. --></p>
<p>Now, suppose in addition that we have covariates on each feature,
<span class="math inline">\(D \in \mathbb{R}^{n \times p}\)</span>,
where <span class="math inline">\(p\)</span> is the dimension of
covariates. For example, <span class="math inline">\(D\)</span> can be a
document-term matrix, where all text data associated with <span class="math inline">\(i\)</span>-th (for <span class="math inline">\(i=1,...,n\)</span>) feature are pooled into a meta
document, and <span class="math inline">\(p\)</span> under this
circumstance is the size of corpus (i.e., total number of words/terms),
and <span class="math inline">\(D_{il}\)</span> is the frequency of word
<span class="math inline">\(l\)</span> (for <span class="math inline">\(l=1,...,p\)</span>) appearing in the <span class="math inline">\(i\)</span>-th document.</p>
<p>The BFF then uses <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> to produce an assessment of covariates
“best” for each cluster. To start with, suppose both <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> has only non-negative entries.Define
the importance, <span class="math inline">\(I \in \mathbb{R}^{p \times
k}\)</span>, of the <span class="math inline">\(l\)</span>-th covariate
to the <span class="math inline">\(j\)</span>-th cluster by the average
of <span class="math inline">\(l\)</span>-th covariate (the <span class="math inline">\(l\)</span>-th columns of <span class="math inline">\(D\)</span>), weighted by the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[I_{lj} = \sum_{j=1}^n D_{jl} Y_{ij},
\text{ for } l=1,...,p,i=1,...n,\]</span></p>
<p>or compactly (side note: the cross product <span class="math inline">\(\langle D,Y \rangle\)</span> is defined as <span class="math inline">\(D^T Y\)</span> as in convention),</p>
<p><span class="math display">\[I=\langle D,Y \rangle.\]</span></p>
<p>As such, a higher value in <span class="math inline">\(I_{lj}\)</span> indicates more significant
importance. BFF selects the “best” covariates for each cluster according
to the <span class="math inline">\(j\)</span>-th (for <span class="math inline">\(j=1, ..., k\)</span>) column of <span class="math inline">\(I\)</span>.</p>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
<p>Below are a few notes on the implementation of BFF:</p>
<ul>
<li><p><strong>Positive skewness</strong>. When <span class="math inline">\(D\)</span> is a document-term matrix (a.k.a., bags
of words), it holds that all elements are non-negative. However, there
is absolutely no guarantee that <span class="math inline">\(Y\)</span>
has all non-negative entries. This motivates the positive-skew
transformation, i.e., we flip the signs of those columns of <span class="math inline">\(Y\)</span> that have negative sample third <a href="https://en.wikipedia.org/wiki/Moment_(mathematics)">moment</a>.</p></li>
<li><p><strong>Handling negative elements</strong>. For now, we undergo
a rather ad-hoc solution to the existence of negative elements in <span class="math inline">\(Y\)</span> – pretending they have little effects.
In the above importance calculation, negative weights (<span class="math inline">\(Y_{ij}&lt;0\)</span>) are equally treated as <span class="math inline">\(-1\)</span>. In other words, the negative elements
result in some subtractions/reduction/contraction in the importance
metrics.</p></li>
<li><p><strong>Weight normalization</strong>. In BFF, we utilize the
<span class="math inline">\(Y\)</span> matrix as a way of weighting
covariates (in <span class="math inline">\(D\)</span>). It is therefore
natrual to expect the columns of <span class="math inline">\(Y\)</span>
to be (akin to) some probability distributions, i.e., the probability to
select one member from the cluster at random. Recall also that the
columns of <span class="math inline">\(Y\)</span> all have (or close to)
unit <span class="math inline">\(\ell_2\)</span>-norm. Hence, additional
transformation is needed: we normalized <span class="math inline">\(Y\)</span> by column. In particular, this is done
separately for positive and negative elements.</p></li>
<li><p><strong>Variance stabilization</strong>. If we model <span class="math inline">\(I_{lj}\)</span> with Poisson rate model <span class="math inline">\(\text{Poisson}(\lambda_{lj})\)</span>, the sample
mean and variance are coupled (i.e., both have the expectation of <span class="math inline">\(\lambda_{lj}\)</span>). In order to standardize
our importance measure <span class="math inline">\(I\)</span>, we need
to decouple these two statistics. Performing a square-root
transformation, <span class="math inline">\(f(x)=\sqrt{x}\)</span>, does
the trick; it stabilizes the sampling variance, which becomes nearly
constant.</p></li>
</ul>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
